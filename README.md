# Stock Movement Prediction System

A machine learning system that predicts stock price movements by analyzing tweets using BERT embeddings and deep learning models (LSTM, GRU, Bidirectional).


## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Validate Setup

```bash
python validate_pipeline.py
```

This runs quick tests to ensure all components work correctly.

### 3. Train Models

```bash
python train.py
```

Options:
- `--epochs N` - Number of training epochs (default: 50)
- `--batch-size N` - Batch size (default: 32)
- `--force-bert` - Recompute BERT embeddings

This will:
- Load and align datasets
- Compute BERT embeddings (cached for future runs)
- Train three models: LSTM, LSTM+GRU, Bidirectional
- Save all models, scaler, and metrics to `model/` directory

### 4. Run Dashboard

```bash
streamlit run app.py
```

The dashboard will be available at http://localhost:8501

## ğŸ“ Project Structure

```
Stock-Movement/
â”œâ”€â”€ Dataset/
â”‚   â”œâ”€â”€ AAPL.csv          # Stock price data
â”‚   â””â”€â”€ tweets.csv         # Tweet data
â”œâ”€â”€ model/                 # Generated by train.py
â”‚   â”œâ”€â”€ bert.npy          # BERT embeddings
â”‚   â”œâ”€â”€ scaler.pkl        # Fitted scaler (prevents data leakage!)
â”‚   â”œâ”€â”€ lstm_model.h5     # LSTM baseline model
â”‚   â”œâ”€â”€ propose_model.h5  # LSTM+GRU hybrid model
â”‚   â”œâ”€â”€ extension_model.h5 # Bidirectional model
â”‚   â”œâ”€â”€ *_history.pckl    # Training histories
â”‚   â””â”€â”€ metrics.json      # Performance metrics
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_alignment.py # Dataset alignment utilities
â”‚   â””â”€â”€ preprocessing.py  # Feature engineering utilities
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_utils.py     # Unit tests
â”œâ”€â”€ train.py              # Training pipeline
â”œâ”€â”€ validate_pipeline.py  # Quick validation script
â”œâ”€â”€ app.py                # Streamlit dashboard
â””â”€â”€ README_DASHBOARD.md   # Dashboard documentation
```

## ğŸ”¬ Technical Details

### Model Input Strategy

The models use a specific input shape for compatibility with the original architecture:

1. **Feature Construction**:
   - BERT embeddings: 768 dimensions (tweet sentiment)
   - Stock features: 4 dimensions (Open, High, Low, Close)
   - Combined: 772 dimensions

2. **Preprocessing**:
   - Normalize using MinMaxScaler (fitted on training data)
   - Skip first 2 features â†’ 770 dimensions
   - Reshape to (35 time steps, 22 features per step)

3. **Why skip first 2 features?**
   - Maintains compatibility with pre-trained models
   - Historical implementation detail preserved
   - Clearly documented in code comments

### Data Alignment Strategy

When tweets have timestamps (recommended):
- Align each tweet to its trading day
- If tweet is on non-trading day â†’ use previous trading day
- Ensures temporal consistency

When tweets lack timestamps (fallback):
- Use index-based pairing
- Assumes data is pre-aligned
- Warns user about limitations

### Preventing Data Leakage

**Problem**: Original code used `fit_transform()` on individual samples during inference, learning from test data.

**Solution**:
1. Fit scaler on training data only
2. Save scaler to `model/scaler.pkl`
3. Load and reuse scaler during inference
4. Never refit on test/production data

## ğŸ“Š Model Architecture

Three models are trained and compared:

1. **LSTM (Baseline)**
   - Single LSTM layer (100 units)
   - Dropout (0.5)
   - Dense layers

2. **LSTM + GRU (Hybrid)**
   - LSTM layer (100 units)
   - GRU layers (80, 64 units)
   - Multiple dropouts (0.2)

3. **Bidirectional (Best)**
   - LSTM layer (100 units)
   - Bidirectional GRU layers
   - Multiple dropouts (0.2)

## ğŸ§ª Testing

Run unit tests:
```bash
python -m pytest tests/ -v
```

Or run validation script:
```bash
python validate_pipeline.py
```

## ğŸ“ˆ Performance Metrics

After training, metrics are saved to `model/metrics.json`:
- Accuracy
- Precision
- Recall
- F1 Score

View in dashboard for detailed comparison.

## âš ï¸ Important Notes

1. **Always train before running dashboard**: The dashboard requires trained models and scaler
2. **Don't delete scaler.pkl**: Critical for prediction consistency
3. **Timestamp data**: Add timestamps to tweets.csv for better alignment
4. **For educational purposes only**: Not financial advice

## ğŸ¤ Contributing

When making changes:
1. Run validation: `python validate_pipeline.py`
2. Run tests: `python -m pytest tests/ -v`
3. Update documentation as needed

## ğŸ“ License

See repository for license information.

## ğŸ™ Credits

- Original implementation: [Pranavvv08](https://github.com/Pranavvv08)
- Fixes and improvements: February 2026 update
- Technologies: TensorFlow, Keras, Sentence Transformers, Streamlit, Plotly
